{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOSSuryIsYP4W2pzW0eckOR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ankushkhakale/-Sign-Language-Recognition-with-PyTorch-CNN/blob/main/ASL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Data Setup (to resolve FileNotFoundError) ---\n",
        "import os\n",
        "import zipfile\n",
        "import shutil\n",
        "\n",
        "# 1. Install Kaggle library\n",
        "!pip install kaggle --quiet\n",
        "\n",
        "# 2. Set up Kaggle API (User needs to upload kaggle.json)\n",
        "# Important: Please upload your kaggle.json file to the Colab environment.\n",
        "# Go to Files (left sidebar) -> .kaggle folder (create if not exists: !mkdir -p ~/.kaggle)\n",
        "# Then upload kaggle.json there.\n",
        "# Ensure correct permissions: !chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "# If you have already uploaded kaggle.json and set permissions, you can comment out the above instructions.\n",
        "\n",
        "# Define dataset path and name\n",
        "KAGGLE_DATASET_NAME = \"grassknoted/asl-alphabet\" # Example ASL Alphabet dataset on Kaggle\n",
        "DOWNLOAD_PATH = \"./data_download\" # Temporary download path\n",
        "TARGET_ROOT = \"./data/asl_alphabet\" # Final structure root for ImageFolder\n",
        "\n",
        "# Create necessary directories\n",
        "os.makedirs(DOWNLOAD_PATH, exist_ok=True)\n",
        "os.makedirs(TARGET_ROOT, exist_ok=True) # Ensure data/asl_alphabet exists\n",
        "\n",
        "# 3. Download the dataset from Kaggle\n",
        "print(f\"Downloading {KAGGLE_DATASET_NAME} dataset...\")\n",
        "# Using a try-except block in case kaggle.json is not set up correctly\n",
        "try:\n",
        "    !kaggle datasets download {KAGGLE_DATASET_NAME} -p {DOWNLOAD_PATH}\n",
        "    print(\"Download complete.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error downloading dataset: {e}\")\n",
        "    print(\"Please ensure your Kaggle API key (kaggle.json) is properly configured in ~/.kaggle.\")\n",
        "    # Raise the error to stop execution if download fails\n",
        "    raise FileNotFoundError(\"Kaggle dataset download failed. Check API key and internet connection.\")\n",
        "\n",
        "# 4. Unzip the dataset\n",
        "# Kaggle typically downloads a single zip for datasets. Assume 'asl-alphabet.zip'\n",
        "zip_file_path = os.path.join(DOWNLOAD_PATH, 'asl-alphabet.zip')\n",
        "\n",
        "# Check for alternative zip file names if 'asl-alphabet.zip' isn't found\n",
        "if not os.path.exists(zip_file_path):\n",
        "    zip_files = [f for f in os.listdir(DOWNLOAD_PATH) if f.endswith('.zip')]\n",
        "    if zip_files:\n",
        "        zip_file_path = os.path.join(DOWNLOAD_PATH, zip_files[0])\n",
        "    else:\n",
        "        raise FileNotFoundError(f\"No zip file found in {DOWNLOAD_PATH} after download.\")\n",
        "\n",
        "print(f\"Unzipping {zip_file_path}...\")\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(DOWNLOAD_PATH) # Extract all into the download path\n",
        "print(\"Unzipping complete.\")\n",
        "\n",
        "# 5. Restructure the dataset to match torchvision.datasets.ImageFolder expectation\n",
        "# The unzipped structure might be: ./data_download/asl_alphabet_train/A/...\n",
        "# We need: ./data/asl_alphabet/train/A/...\n",
        "\n",
        "source_train_dir = os.path.join(DOWNLOAD_PATH, 'asl_alphabet_train')\n",
        "target_train_dir = os.path.join(TARGET_ROOT, 'train')\n",
        "\n",
        "source_test_dir = os.path.join(DOWNLOAD_PATH, 'asl_alphabet_test')\n",
        "target_test_dir = os.path.join(TARGET_ROOT, 'test')\n",
        "\n",
        "# Move 'asl_alphabet_train' to 'data/asl_alphabet/train'\n",
        "if os.path.exists(source_train_dir):\n",
        "    print(f\"Moving '{source_train_dir}' to '{target_train_dir}'...\")\n",
        "    shutil.move(source_train_dir, target_train_dir)\n",
        "else:\n",
        "    print(f\"Warning: '{source_train_dir}' not found. Check unzipped directory structure.\")\n",
        "\n",
        "# Move 'asl_alphabet_test' to 'data/asl_alphabet/test'\n",
        "if os.path.exists(source_test_dir):\n",
        "    print(f\"Moving '{source_test_dir}' to '{target_test_dir}'...\")\n",
        "    shutil.move(source_test_dir, target_test_dir)\n",
        "else:\n",
        "    print(f\"Warning: '{source_test_dir}' not found. Check unzipped directory structure.\")\n",
        "\n",
        "# Clean up temporary download folder (optional)\n",
        "# shutil.rmtree(DOWNLOAD_PATH)\n",
        "print(\"Dataset preparation complete.\")\n",
        "# --- End Data Setup ---\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "from torchvision import transforms, datasets\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((64, 64)),\n",
        "    transforms.Grayscale(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "# Load ASL Alphabet Dataset (e.g., ASL Alphabet dataset from Kaggle)\n",
        "# Directory structure: data/asl_alphabet/A/, B/, ..., Z/\n",
        "train_dataset = datasets.ImageFolder(root='./data/asl_alphabet/train', transform=transform)\n",
        "test_dataset = datasets.ImageFolder(root='./data/asl_alphabet/test', transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64)\n",
        "\n",
        "# CNN Model\n",
        "class SignLanguageCNN(nn.Module):\n",
        "    def __init__(self, num_classes=26):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv2d(1, 32, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2),\n",
        "            nn.Conv2d(32, 64, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(64 * 16 * 16, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "model = SignLanguageCNN(num_classes=26)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(5):\n",
        "    for images, labels in train_loader:\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "# Evaluation\n",
        "correct, total = 0, 0\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        outputs = model(images)\n",
        "        preds = torch.argmax(outputs, dim=1)\n",
        "        correct += (preds == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "print(f\"Test Accuracy: {100 * correct / total:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        },
        "id": "rxIVPJWM895n",
        "outputId": "2da5ff81-f8fc-4320-f9a4-1dc002806b5d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading grassknoted/asl-alphabet dataset...\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/kaggle\", line 10, in <module>\n",
            "    sys.exit(main())\n",
            "             ^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/kaggle/cli.py\", line 68, in main\n",
            "    out = args.func(**command_args)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/kaggle/api/kaggle_api_extended.py\", line 1741, in dataset_download_cli\n",
            "    with self.build_kaggle_client() as kaggle:\n",
            "         ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/kaggle/api/kaggle_api_extended.py\", line 688, in build_kaggle_client\n",
            "    username=self.config_values['username'],\n",
            "             ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^\n",
            "KeyError: 'username'\n",
            "Download complete.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "No zip file found in ./data_download after download.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-674989613.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mzip_file_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDOWNLOAD_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzip_files\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"No zip file found in {DOWNLOAD_PATH} after download.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Unzipping {zip_file_path}...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: No zip file found in ./data_download after download."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "s0MsjxeY_qLr"
      }
    }
  ]
}